{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8fcaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, math, random, string, unicodedata\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "\n",
    "DATA_DIR   = Path(\"../data\")\n",
    "REPORTS    = Path(\"../reports\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "CONF_DIR   = Path(\"../configs\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CONF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_CSV  = DATA_DIR / \"processed_clean.csv\"     # del EDA\n",
    "OUT_PROCES = DATA_DIR / \"processed.csv\"           # con columnas binarias por clase\n",
    "OUT_CLASSES= DATA_DIR / \"classes.txt\"             # lista de clases\n",
    "SPLIT_PATH = DATA_DIR / \"splits.json\"             # índices de train/val/test\n",
    "VEC_PATH   = MODELS_DIR / \"baseline\" / \"tfidf.joblib\"  \n",
    "(Path(MODELS_DIR / \"baseline\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Config OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8939ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3563, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>group</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Real-world\" data on the efficacy and safety o...</td>\n",
       "      <td>Lenalidomide and dexamethasone (RD) is a stand...</td>\n",
       "      <td>neurological</td>\n",
       "      <td>\"Real-world\" data on the efficacy and safety o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>BACKGROUND: Calcitriol therapy suppresses seru...</td>\n",
       "      <td>hepatorenal</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  \"Real-world\" data on the efficacy and safety o...   \n",
       "1  22-oxacalcitriol suppresses secondary hyperpar...   \n",
       "\n",
       "                                            abstract         group  \\\n",
       "0  Lenalidomide and dexamethasone (RD) is a stand...  neurological   \n",
       "1  BACKGROUND: Calcitriol therapy suppresses seru...   hepatorenal   \n",
       "\n",
       "                                                text  \n",
       "0  \"Real-world\" data on the efficacy and safety o...  \n",
       "1  22-oxacalcitriol suppresses secondary hyperpar...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert INPUT_CSV.exists(), f\"No encuentro {INPUT_CSV}. Genera este archivo al final del 01_eda.ipynb.\"\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6bc5cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '..\\\\data\\\\processed_clean.csv',\n",
       " 'rows': 3563,\n",
       " 'columns': ['title', 'abstract', 'group', 'text'],\n",
       " 'note': 'Archivo limpio exportado desde 01_eda.ipynb tras eliminar duplicados y fusionar por title.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validaciones fuertes del esquema\n",
    "expected_cols = {\"title\",\"abstract\",\"group\",\"text\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "assert not missing, f\"Faltan columnas: {missing}\"\n",
    "\n",
    "assert df[\"title\"].isna().sum()==0, \"title tiene NaN\"\n",
    "assert df[\"abstract\"].isna().sum()==0, \"abstract tiene NaN\"\n",
    "assert df[\"group\"].isna().sum()==0, \"group tiene NaN\"\n",
    "assert len(df) >= 3000, \"Tamaño inesperado; revisa el EDA/limpieza\"\n",
    "\n",
    "# Documentar procedencia en un archivo\n",
    "provenance = {\n",
    "    \"source\": str(INPUT_CSV),\n",
    "    \"rows\": int(len(df)),\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"note\": \"Archivo limpio exportado desde 01_eda.ipynb tras eliminar duplicados y fusionar por title.\",\n",
    "}\n",
    "with open(REPORTS / \"provenance_preprocessing.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(provenance, f, indent=2, ensure_ascii=False)\n",
    "provenance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2072c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisiones de preprocesamiento:\n",
      "1) Unificamos 'title' + 'abstract' en 'text' (hecho en el EDA) para simplificar la entrada del modelo.\n",
      "2) Normalizamos texto de forma mínima y segura para datos biomédicos:\n",
      "   - Minusculización y limpieza de espacios.\n",
      "   - Conservamos dígitos, guiones y términos clínicos (no removemos agresivamente).\n",
      "   - Evitamos stemming/lemmatization fuerte para no romper terminología médica.\n",
      "3) Binarizamos etiquetas multilabel con un listado determinístico de clases.\n",
      "4) Dividimos train/val/test con estratificación **multilabel** (iterative-stratification) y semilla fija.\n",
      "5) Generamos TF-IDF aquí si el baseline clásico lo requiere, para ahorrar tiempo en el entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "justificacion = dedent(\"\"\"\n",
    "Decisiones de preprocesamiento:\n",
    "1) Unificamos 'title' + 'abstract' en 'text' (hecho en el EDA) para simplificar la entrada del modelo.\n",
    "2) Normalizamos texto de forma mínima y segura para datos biomédicos:\n",
    "   - Minusculización y limpieza de espacios.\n",
    "   - Conservamos dígitos, guiones y términos clínicos (no removemos agresivamente).\n",
    "   - Evitamos stemming/lemmatization fuerte para no romper terminología médica.\n",
    "3) Binarizamos etiquetas multilabel con un listado determinístico de clases.\n",
    "4) Dividimos train/val/test con estratificación **multilabel** (iterative-stratification) y semilla fija.\n",
    "5) Generamos TF-IDF aquí si el baseline clásico lo requiere, para ahorrar tiempo en el entrenamiento.\n",
    "\"\"\").strip()\n",
    "print(justificacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ba6a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      text                                                                                  text_norm\n",
      "\"Real-world\" data on the efficacy and safety of lenalidomide and dexamethasone in patie... \"real-world\" data on the efficacy and safety of lenalidomide and dexamethasone in patie...\n"
     ]
    }
   ],
   "source": [
    "# Nota: Normalización minimalista—evita destruir términos como \"BRCA1\", \"TNF-α\", etc.\n",
    "# Convertimos a minúsculas, colapsamos espacios, removemos caracteres invisibles raros.\n",
    "# Preservamos dígitos y signos relevantes (%, -, /) que aparecen en resúmenes.\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    # Unicode NFKC: normaliza formas de caracteres\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    # Minúsculas\n",
    "    s = s.lower()\n",
    "    # Reemplazar múltiples espacios y saltos por un espacio\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"text_norm\"] = df[\"text\"].apply(normalize_text)\n",
    "\n",
    "# Comprobación rápida\n",
    "print(df[[\"text\",\"text_norm\"]].head(1).to_string(index=False, max_colwidth=90))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "383f399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases: ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']\n",
      "{'cardiovascular': 1267, 'hepatorenal': 1091, 'neurological': 1784, 'oncological': 600}\n",
      "Guardado: ..\\data\\classes.txt\n"
     ]
    }
   ],
   "source": [
    "# Listas de etiquetas por fila\n",
    "labels_series = df[\"group\"].astype(str).apply(lambda s: [t.strip() for t in s.split(\"|\") if t.strip()!=\"\"])\n",
    "\n",
    "# Conjunto de clases (orden determinista)\n",
    "classes = sorted({lab for L in labels_series for lab in L})\n",
    "print(\"Clases:\", classes)\n",
    "\n",
    "# Matriz binaria Y (n x C)\n",
    "Y = pd.DataFrame([{c: int(c in L) for c in classes} for L in labels_series], dtype=int)\n",
    "print(Y.sum().to_dict())  # positivos por clase\n",
    "\n",
    "# Guardar lista de clases\n",
    "with open(OUT_CLASSES, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(classes))\n",
    "print(\"Guardado:\", OUT_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3406641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: ..\\data\\processed.csv | shape: (3563, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>group</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>cardiovascular</th>\n",
       "      <th>hepatorenal</th>\n",
       "      <th>neurological</th>\n",
       "      <th>oncological</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Real-world\" data on the efficacy and safety o...</td>\n",
       "      <td>Lenalidomide and dexamethasone (RD) is a stand...</td>\n",
       "      <td>neurological</td>\n",
       "      <td>\"Real-world\" data on the efficacy and safety o...</td>\n",
       "      <td>\"real-world\" data on the efficacy and safety o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>BACKGROUND: Calcitriol therapy suppresses seru...</td>\n",
       "      <td>hepatorenal</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  \"Real-world\" data on the efficacy and safety o...   \n",
       "1  22-oxacalcitriol suppresses secondary hyperpar...   \n",
       "\n",
       "                                            abstract         group  \\\n",
       "0  Lenalidomide and dexamethasone (RD) is a stand...  neurological   \n",
       "1  BACKGROUND: Calcitriol therapy suppresses seru...   hepatorenal   \n",
       "\n",
       "                                                text  \\\n",
       "0  \"Real-world\" data on the efficacy and safety o...   \n",
       "1  22-oxacalcitriol suppresses secondary hyperpar...   \n",
       "\n",
       "                                           text_norm  cardiovascular  \\\n",
       "0  \"real-world\" data on the efficacy and safety o...               0   \n",
       "1  22-oxacalcitriol suppresses secondary hyperpar...               0   \n",
       "\n",
       "   hepatorenal  neurological  oncological  \n",
       "0            0             1            0  \n",
       "1            1             0            0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pd.concat([df[[\"title\",\"abstract\",\"group\",\"text\",\"text_norm\"]], Y], axis=1)\n",
    "out.to_csv(OUT_PROCES, index=False)\n",
    "print(\"Guardado:\", OUT_PROCES, \"| shape:\", out.shape)\n",
    "out.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ba15a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val: 2963  | Test: 600\n",
      "Guardado: ..\\data\\splits.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(600, [2368, 2373, 2368, 2369, 2374], [595, 590, 595, 594, 589])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install iterative-stratification antes de correr\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Estrategia: primero separo TEST (15%), luego CV 5-fold sobre el 85% restante para entrenar y validar\n",
    "TEST_SIZE = 0.15\n",
    "N_SPLITS  = 5\n",
    "\n",
    "indices = np.arange(len(out))\n",
    "X_dummy  = np.zeros((len(out), 1))  # no se usa, pero requerido\n",
    "Y_mat    = out[classes].values\n",
    "\n",
    "# Split holdout test estratificado\n",
    "idx_trainval, idx_test = next(\n",
    "    MultilabelStratifiedKFold(n_splits=int(1/TEST_SIZE), shuffle=True, random_state=SEED).split(X_dummy, Y_mat)\n",
    ")\n",
    "# Lo anterior crea 1 fold como \"test\". Ajustamos por si no cae exacto al 15%\n",
    "idx_trainval = np.array(idx_trainval)\n",
    "idx_test     = np.array(idx_test)\n",
    "\n",
    "print(\"Train+Val:\", len(idx_trainval), \" | Test:\", len(idx_test))\n",
    "\n",
    "# Ahora CV 5-fold dentro de train/val\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "folds = []\n",
    "for fold_id, (tr, va) in enumerate(mskf.split(X_dummy[idx_trainval], Y_mat[idx_trainval])):\n",
    "    folds.append({\n",
    "        \"fold\": fold_id,\n",
    "        \"train_idx\": idx_trainval[tr].tolist(),\n",
    "        \"val_idx\":   idx_trainval[va].tolist()\n",
    "    })\n",
    "\n",
    "splits = {\n",
    "    \"seed\": SEED,\n",
    "    \"test_idx\": idx_test.tolist(),\n",
    "    \"folds\": folds,\n",
    "    \"classes\": classes\n",
    "}\n",
    "with open(SPLIT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(splits, f, indent=2)\n",
    "print(\"Guardado:\", SPLIT_PATH)\n",
    "len(idx_test), [len(f[\"train_idx\"]) for f in folds], [len(f[\"val_idx\"]) for f in folds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b43bb2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos MultilabelStratifiedKFold para mantener las proporciones por clase en cada split. \n",
    "# Primero separamos un test estable (evaluación final), y luego hacemos 5 folds para ajustar el modelo y umbrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b147271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (3563, 23621)\n",
      "Guardado vectorizador: ..\\models\\baseline\\tfidf.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_features=50000,\n",
    "    min_df=3,\n",
    "    sublinear_tf=True,     \n",
    "    lowercase=False        \n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(out[\"text_norm\"].astype(str).values)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n",
    "joblib.dump(tfidf, VEC_PATH)\n",
    "print(\"Guardado vectorizador:\", VEC_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ea70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (1–2-gramas, min_df=3, sublinear_tf=True) rinde fuerte como baseline en textos clínicos. \n",
    "# Guardamos el vectorizador para reutilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27aa38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: ..\\data\\sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Pequeña muestra anónima para que terceros prueben el pipeline sin datos completos\n",
    "sample = out.sample(n=min(200, len(out)), random_state=SEED)  # 200 filas\n",
    "sample.to_csv(DATA_DIR / \"sample.csv\", index=False)\n",
    "print(\"Guardado:\", DATA_DIR / \"sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69637649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: ..\\configs\\preprocessing_config.json\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"seed\": SEED,\n",
    "    \"input\": str(INPUT_CSV),\n",
    "    \"outputs\": {\n",
    "        \"processed_csv\": str(OUT_PROCES),\n",
    "        \"classes_txt\": str(OUT_CLASSES),\n",
    "        \"splits_json\": str(SPLIT_PATH),\n",
    "        \"tfidf_vectorizer\": str(VEC_PATH)\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"normalization\": \"unicode NFKC, lowercase, trim spaces\",\n",
    "        \"field_used\": \"text_norm\",\n",
    "        \"keep_digits\": True,\n",
    "        \"avoid_aggressive_stemming\": True\n",
    "    },\n",
    "    \"labels\": classes,\n",
    "    \"split\": {\n",
    "        \"test_size_approx\": 0.15,\n",
    "        \"n_splits_cv\": 5,\n",
    "        \"stratification\": \"MultilabelStratifiedKFold\"\n",
    "    },\n",
    "    \"tfidf\": {\n",
    "        \"enabled\": True,\n",
    "        \"ngram_range\": [1,2],\n",
    "        \"max_features\": 50000,\n",
    "        \"min_df\": 3,\n",
    "        \"sublinear_tf\": True,\n",
    "        \"lowercase\": False\n",
    "    }\n",
    "}\n",
    "with open(CONF_DIR / \"preprocessing_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "print(\"Guardado:\", CONF_DIR / \"preprocessing_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2e95446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': 3563,\n",
       " 'cols': 9,\n",
       " 'classes': ['cardiovascular', 'hepatorenal', 'neurological', 'oncological'],\n",
       " 'positives_per_class': {'cardiovascular': 1267,\n",
       "  'hepatorenal': 1091,\n",
       "  'neurological': 1784,\n",
       "  'oncological': 600},\n",
       " 'has_text_norm': True,\n",
       " 'files': {'processed_csv': 'True',\n",
       "  'classes_txt': 'True',\n",
       "  'splits_json': 'True',\n",
       "  'tfidf_vectorizer': 'True'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = {\n",
    "    \"rows\": int(len(out)),\n",
    "    \"cols\": out.shape[1],\n",
    "    \"classes\": classes,\n",
    "    \"positives_per_class\": {c: int(out[c].sum()) for c in classes},\n",
    "    \"has_text_norm\": \"text_norm\" in out.columns,\n",
    "    \"files\": {\n",
    "        \"processed_csv\": str(OUT_PROCES.exists()),\n",
    "        \"classes_txt\": str(OUT_CLASSES.exists()),\n",
    "        \"splits_json\": str(SPLIT_PATH.exists()),\n",
    "        \"tfidf_vectorizer\": str(VEC_PATH.exists())\n",
    "    }\n",
    "}\n",
    "report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
