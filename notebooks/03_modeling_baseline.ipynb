{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a9837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3563,\n",
       " (3563, 23621),\n",
       " ['cardiovascular', 'hepatorenal', 'neurological', 'oncological'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "FAST = True            \n",
    "N_FOLDS_FAST = 3       # folds para ranking\n",
    "MAX_ITER_LR = 1000     # iters para Logistic\n",
    "N_JOBS = -1\n",
    "\n",
    "\n",
    "DATA = Path(\"../data\")\n",
    "MODELS_BASE = Path(\"../models/baseline\");  MODELS_BASE.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_OUT  = Path(\"../models/experiments\"); MODELS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS     = Path(\"../reports\"); REPORTS.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA/\"processed.csv\")                # requiere 'text_norm'\n",
    "with open(DATA/\"classes.txt\") as f: classes = [l.strip() for l in f if l.strip()]\n",
    "with open(DATA/\"splits.json\") as f: splits = json.load(f)\n",
    "\n",
    "\n",
    "tfidf_path = MODELS_BASE/\"tfidf.joblib\"\n",
    "if not tfidf_path.exists():\n",
    "    raise FileNotFoundError(\"No encuentro ../models/baseline/tfidf.joblib (corre 02_preprocess primero).\")\n",
    "tfidf = joblib.load(tfidf_path)\n",
    "\n",
    "\n",
    "X = tfidf.transform(df[\"text_norm\"].astype(str))\n",
    "Y = df[classes].values.astype(int)\n",
    "\n",
    "\n",
    "test_idx = np.array(splits[\"test_idx\"])\n",
    "trainval_idx = np.setdiff1d(np.arange(len(df)), test_idx)\n",
    "\n",
    "len(df), X.shape, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def sweep_thresholds(y_true, y_score, grid=np.arange(0.05, 0.96, 0.01)):\n",
    "    \"\"\"Busca umbral por clase maximizando F1 (rápido, robusto).\"\"\"\n",
    "    C = y_true.shape[1]; thr = np.zeros(C)\n",
    "    for j in range(C):\n",
    "        s = y_score[:, j]; yt = y_true[:, j]\n",
    "        best_f1, best_t = -1, 0.5\n",
    "        for t in grid:\n",
    "            f1 = f1_score(yt, (s >= t).astype(int), zero_division=0)\n",
    "            if f1 > best_f1: best_f1, best_t = f1, t\n",
    "        thr[j] = best_t\n",
    "    return thr\n",
    "\n",
    "def to_proba(model, Xsub):\n",
    "    \"\"\"\n",
    "    Devuelve probabilidades para multilabel.\n",
    "    - Si hay predict_proba: usa proba[:,1] por clase (OVR).\n",
    "    - Si hay decision_function: reescala a [0,1] con MinMax (rápido, evita calibración pesada).\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(Xsub)\n",
    "        if isinstance(p, list):  # OVR con lista por clase\n",
    "            p = np.vstack([col[:, 1] for col in p]).T\n",
    "        return p\n",
    "    scores = model.decision_function(Xsub)\n",
    "    return MinMaxScaler().fit_transform(scores)\n",
    "\n",
    "def metrics_from(y_true, y_score, thr):\n",
    "    y_pred = (y_score >= thr).astype(int)\n",
    "    m = {\n",
    "        \"f1_micro\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"roc_auc_macro\": roc_auc_score(y_true, y_score, average=\"macro\"),\n",
    "        \"pr_auc_macro\":  average_precision_score(y_true, y_score, average=\"macro\"),\n",
    "    }\n",
    "    # detalle por clase (útil para informe)\n",
    "    m[\"per_class\"] = {}\n",
    "    for i, c in enumerate(classes):\n",
    "        m[\"per_class\"][c] = {\n",
    "            \"f1\": f1_score(y_true[:,i], y_pred[:,i], zero_division=0),\n",
    "            \"roc_auc\": roc_auc_score(y_true[:,i], y_score[:,i]),\n",
    "            \"pr_auc\": average_precision_score(y_true[:,i], y_score[:,i]),\n",
    "            \"thr\": float(thr[i]),\n",
    "        }\n",
    "    return m, y_pred\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f: json.dump(obj, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def get_models():\n",
    "    return {\n",
    "        # baseline fuerte\n",
    "        \"logreg_C2\": OneVsRestClassifier(LogisticRegression(\n",
    "            solver=\"saga\", penalty=\"l2\", C=2.0, max_iter=MAX_ITER_LR,\n",
    "            n_jobs=N_JOBS, class_weight=\"balanced\", random_state=42\n",
    "        ), n_jobs=N_JOBS),\n",
    "\n",
    "        # muy competitivo con TF-IDF\n",
    "        \"ridge_a1\": OneVsRestClassifier(RidgeClassifier(\n",
    "            alpha=1.0, class_weight=\"balanced\", random_state=42\n",
    "        ), n_jobs=N_JOBS),\n",
    "\n",
    "        # SVM lineal sin calibración \n",
    "        \"linsvm_C1\": OneVsRestClassifier(LinearSVC(\n",
    "            C=1.0, random_state=42\n",
    "        ), n_jobs=N_JOBS),\n",
    "\n",
    "        # Naive Bayes complement \n",
    "        \"cnb_a05\": OneVsRestClassifier(ComplementNB(alpha=0.5), n_jobs=N_JOBS),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "FOLDS_FAST  = [ (np.array(f[\"train_idx\"]), np.array(f[\"val_idx\"])) for f in splits[\"folds\"][:N_FOLDS_FAST] ]\n",
    "FOLDS_FULL  = [ (np.array(f[\"train_idx\"]), np.array(f[\"val_idx\"])) for f in splits[\"folds\"] ]\n",
    "\n",
    "def cv_experiment(name, model, folds):\n",
    "    t0 = time.time()\n",
    "    fold_thrs, fold_metrics = [], []\n",
    "\n",
    "    for tr, va in folds:\n",
    "        mdl = clone(model)\n",
    "        mdl.fit(X[tr], Y[tr])\n",
    "        proba = to_proba(mdl, X[va])\n",
    "        thr = sweep_thresholds(Y[va], proba)\n",
    "        m,_ = metrics_from(Y[va], proba, thr)\n",
    "        fold_thrs.append(thr); fold_metrics.append(m)\n",
    "\n",
    "    thr_med = np.median(np.array(fold_thrs), axis=0)\n",
    "\n",
    "\n",
    "    mdl_final = clone(model).fit(X[trainval_idx], Y[trainval_idx])\n",
    "    test_proba = to_proba(mdl_final, X[test_idx])\n",
    "    test_m, _ = metrics_from(Y[test_idx], test_proba, thr_med)\n",
    "\n",
    "    elapsed = round((time.time() - t0)/60, 2)\n",
    "    res = {\n",
    "        \"name\": name,\n",
    "        \"cv_f1_micro\": float(np.mean([m[\"f1_micro\"] for m in fold_metrics])),\n",
    "        \"cv_f1_macro\": float(np.mean([m[\"f1_macro\"] for m in fold_metrics])),\n",
    "        \"test_f1_micro\": test_m[\"f1_micro\"],\n",
    "        \"test_f1_macro\": test_m[\"f1_macro\"],\n",
    "        \"minutes\": elapsed,\n",
    "        \"thr\": thr_med.tolist(),\n",
    "    }\n",
    "    return res, mdl_final, thr_med, test_proba, test_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> logreg_C2  (FAST)\n",
      "\n",
      ">>> ridge_a1  (FAST)\n",
      "\n",
      ">>> linsvm_C1  (FAST)\n",
      "\n",
      ">>> cnb_a05  (FAST)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cv_f1_micro</th>\n",
       "      <th>cv_f1_macro</th>\n",
       "      <th>test_f1_micro</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>minutes</th>\n",
       "      <th>thr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linsvm_C1</td>\n",
       "      <td>0.897535</td>\n",
       "      <td>0.890769</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.870591</td>\n",
       "      <td>0.06</td>\n",
       "      <td>[0.36000000000000004, 0.34, 0.4700000000000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg_C2</td>\n",
       "      <td>0.887863</td>\n",
       "      <td>0.880181</td>\n",
       "      <td>0.868505</td>\n",
       "      <td>0.855164</td>\n",
       "      <td>0.18</td>\n",
       "      <td>[0.4600000000000001, 0.4600000000000001, 0.510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ridge_a1</td>\n",
       "      <td>0.891695</td>\n",
       "      <td>0.883198</td>\n",
       "      <td>0.876933</td>\n",
       "      <td>0.848997</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[0.4100000000000001, 0.38000000000000006, 0.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnb_a05</td>\n",
       "      <td>0.818879</td>\n",
       "      <td>0.776630</td>\n",
       "      <td>0.807575</td>\n",
       "      <td>0.758549</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[0.5000000000000001, 0.43000000000000005, 0.05...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  cv_f1_micro  cv_f1_macro  test_f1_micro  test_f1_macro  minutes  \\\n",
       "2  linsvm_C1     0.897535     0.890769       0.886076       0.870591     0.06   \n",
       "0  logreg_C2     0.887863     0.880181       0.868505       0.855164     0.18   \n",
       "1   ridge_a1     0.891695     0.883198       0.876933       0.848997     0.05   \n",
       "3    cnb_a05     0.818879     0.776630       0.807575       0.758549     0.04   \n",
       "\n",
       "                                                 thr  \n",
       "2  [0.36000000000000004, 0.34, 0.4700000000000001...  \n",
       "0  [0.4600000000000001, 0.4600000000000001, 0.510...  \n",
       "1  [0.4100000000000001, 0.38000000000000006, 0.51...  \n",
       "3  [0.5000000000000001, 0.43000000000000005, 0.05...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RUN = [\"logreg_C2\", \"ridge_a1\", \"linsvm_C1\", \"cnb_a05\"]  \n",
    "\n",
    "folds = FOLDS_FAST if FAST else FOLDS_FULL\n",
    "results, artifacts = [], {}\n",
    "\n",
    "for name in RUN:\n",
    "    print(f\"\\n>>> {name}  ({'FAST' if FAST else 'FULL'})\")\n",
    "    res, mdl, thr, test_proba, test_m = cv_experiment(name, get_models()[name], folds)\n",
    "    results.append(res)\n",
    "    artifacts[name] = {\"model\": mdl, \"thr\": np.array(thr), \"test_proba\": test_proba, \"test_metrics\": test_m}\n",
    "    pd.DataFrame(results).to_csv(REPORTS/\"experiments_rank_partial.csv\", index=False)\n",
    "\n",
    "rank_df = pd.DataFrame(results).sort_values(\"test_f1_macro\", ascending=False)\n",
    "display(rank_df)\n",
    "rank_df.to_csv(REPORTS/\"experiments_rank.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb650c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-2 para tuning: ['linsvm_C1', 'logreg_C2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cv_f1_micro</th>\n",
       "      <th>cv_f1_macro</th>\n",
       "      <th>test_f1_micro</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>minutes</th>\n",
       "      <th>thr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linsvm_C1_C0.5</td>\n",
       "      <td>0.896595</td>\n",
       "      <td>0.890018</td>\n",
       "      <td>0.895086</td>\n",
       "      <td>0.886949</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[0.36000000000000004, 0.35000000000000003, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linsvm_C1_C1.0</td>\n",
       "      <td>0.897535</td>\n",
       "      <td>0.890769</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.870591</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[0.36000000000000004, 0.34, 0.4700000000000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linsvm_C1_C2.0</td>\n",
       "      <td>0.896414</td>\n",
       "      <td>0.889009</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>0.865607</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[0.38000000000000006, 0.34, 0.4600000000000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logreg_C2_C4.0</td>\n",
       "      <td>0.890892</td>\n",
       "      <td>0.882074</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>0.865032</td>\n",
       "      <td>0.12</td>\n",
       "      <td>[0.44000000000000006, 0.42000000000000004, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logreg_C2_C1.0</td>\n",
       "      <td>0.878568</td>\n",
       "      <td>0.872639</td>\n",
       "      <td>0.868096</td>\n",
       "      <td>0.856712</td>\n",
       "      <td>0.06</td>\n",
       "      <td>[0.4600000000000001, 0.4700000000000001, 0.520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logreg_C2_C2.0</td>\n",
       "      <td>0.887863</td>\n",
       "      <td>0.880181</td>\n",
       "      <td>0.868505</td>\n",
       "      <td>0.855164</td>\n",
       "      <td>0.08</td>\n",
       "      <td>[0.4600000000000001, 0.4600000000000001, 0.510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg_C2_C0.5</td>\n",
       "      <td>0.863552</td>\n",
       "      <td>0.858972</td>\n",
       "      <td>0.853033</td>\n",
       "      <td>0.840248</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[0.4600000000000001, 0.4700000000000001, 0.460...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  cv_f1_micro  cv_f1_macro  test_f1_micro  test_f1_macro  \\\n",
       "0  linsvm_C1_C0.5     0.896595     0.890018       0.895086       0.886949   \n",
       "1  linsvm_C1_C1.0     0.897535     0.890769       0.886076       0.870591   \n",
       "2  linsvm_C1_C2.0     0.896414     0.889009       0.882548       0.865607   \n",
       "6  logreg_C2_C4.0     0.890892     0.882074       0.875800       0.865032   \n",
       "4  logreg_C2_C1.0     0.878568     0.872639       0.868096       0.856712   \n",
       "5  logreg_C2_C2.0     0.887863     0.880181       0.868505       0.855164   \n",
       "3  logreg_C2_C0.5     0.863552     0.858972       0.853033       0.840248   \n",
       "\n",
       "   minutes                                                thr  \n",
       "0     0.05  [0.36000000000000004, 0.35000000000000003, 0.4...  \n",
       "1     0.05  [0.36000000000000004, 0.34, 0.4700000000000001...  \n",
       "2     0.05  [0.38000000000000006, 0.34, 0.4600000000000001...  \n",
       "6     0.12  [0.44000000000000006, 0.42000000000000004, 0.5...  \n",
       "4     0.06  [0.4600000000000001, 0.4700000000000001, 0.520...  \n",
       "5     0.08  [0.4600000000000001, 0.4600000000000001, 0.510...  \n",
       "3     0.05  [0.4600000000000001, 0.4700000000000001, 0.460...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOP_K = min(2, len(rank_df))\n",
    "candidates = rank_df.head(TOP_K)[\"name\"].tolist()\n",
    "print(\"Top-2 para tuning:\", candidates)\n",
    "\n",
    "param_grid = {\n",
    "    \"logreg_C2\": [{\"C\": c} for c in [0.5, 1.0, 2.0, 4.0]],\n",
    "    \"ridge_a1\" : [{\"alpha\": a} for a in [0.5, 1.0, 2.0]],\n",
    "    \"linsvm_C1\":[{\"C\": c} for c in [0.5, 1.0, 2.0]],\n",
    "    \"cnb_a05\"  : [{\"alpha\": a} for a in [0.1, 0.5, 1.0]],\n",
    "}\n",
    "\n",
    "def set_params(base, p):\n",
    "    \"\"\"Crea una copia del modelo de get_models con hiperparámetros p.\"\"\"\n",
    "    from sklearn.base import clone\n",
    "    m = get_models()[base]\n",
    "    est = clone(m.estimator)\n",
    "    est.set_params(**p)\n",
    "    return m.set_params(estimator=est)\n",
    "\n",
    "tune_res = []\n",
    "for base in candidates:\n",
    "    grid = param_grid.get(base, [{}])\n",
    "    for p in grid:\n",
    "        name = f\"{base}_{'_'.join([f'{k}{v}' for k,v in p.items()])}\"\n",
    "        res, mdl, thr, test_proba, test_m = cv_experiment(name, set_params(base, p), FOLDS_FAST)\n",
    "        tune_res.append(res)\n",
    "\n",
    "tune_df = pd.DataFrame(tune_res).sort_values(\"test_f1_macro\", ascending=False)\n",
    "display(tune_df)\n",
    "tune_df.to_csv(REPORTS/\"experiments_tuning.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867924ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para ensemble: ['linsvm_C1', 'logreg_C2', 'ridge_a1']\n",
      "ENSEMBLE — test_f1_micro: 0.8882  test_f1_macro: 0.8795\n"
     ]
    }
   ],
   "source": [
    "TOP_ENSEMBLE = 3\n",
    "top_names = rank_df.head(TOP_ENSEMBLE)[\"name\"].tolist()\n",
    "print(\"Para ensemble:\", top_names)\n",
    "ens = []\n",
    "for name in top_names:\n",
    "    if name in artifacts:\n",
    "        ens.append(artifacts[name])\n",
    "    else:\n",
    "        res, mdl, thr, test_proba, test_m = cv_experiment(name, get_models()[name], FOLDS_FAST)\n",
    "        ens.append({\"model\": mdl, \"thr\": np.array(thr), \"test_proba\": test_proba, \"test_metrics\": test_m})\n",
    "\n",
    "probas = [e[\"test_proba\"] for e in ens]\n",
    "P = np.mean(probas, axis=0)\n",
    "THR_ENS = np.median(np.vstack([e[\"thr\"] for e in ens]), axis=0)\n",
    "\n",
    "m_ens, y_pred_ens = metrics_from(Y[test_idx], P, THR_ENS)\n",
    "print(\"ENSEMBLE — test_f1_micro:\", round(m_ens[\"f1_micro\"],4), \" test_f1_macro:\", round(m_ens[\"f1_macro\"],4))\n",
    "\n",
    "save_json(m_ens, REPORTS/\"ensemble_test_metrics.json\")\n",
    "save_json({c: float(t) for c,t in zip(classes, THR_ENS)}, MODELS_OUT/\"ensemble_thresholds.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor single (tuning o rank): linsvm_C1_C0.5\n",
      "FULL — test_f1_micro: 0.8933  test_f1_macro: 0.8855\n",
      "GANADOR: linsvm_C1_C0.5_full5fold | macro-F1: 0.8855\n",
      "Artefactos en: ..\\models\\experiments\\linsvm_C1_C0.5_full5fold\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('linsvm_C1_C0.5_full5fold',\n",
       " {'f1_micro': 0.8932547478716437,\n",
       "  'f1_macro': 0.8854984194294601,\n",
       "  'roc_auc_macro': np.float64(0.9618312768809928),\n",
       "  'pr_auc_macro': np.float64(0.9432351466086362),\n",
       "  'per_class': {'cardiovascular': {'f1': 0.9158415841584159,\n",
       "    'roc_auc': np.float64(0.9768150196761656),\n",
       "    'pr_auc': np.float64(0.9706234965926278),\n",
       "    'thr': 0.35000000000000003},\n",
       "   'hepatorenal': {'f1': 0.8672566371681416,\n",
       "    'roc_auc': np.float64(0.9521662547978339),\n",
       "    'pr_auc': np.float64(0.9372755327073047),\n",
       "    'thr': 0.35000000000000003},\n",
       "   'neurological': {'f1': 0.9048414023372288,\n",
       "    'roc_auc': np.float64(0.9716638330499716),\n",
       "    'pr_auc': np.float64(0.9705105556858418),\n",
       "    'thr': 0.4600000000000001},\n",
       "   'oncological': {'f1': 0.8540540540540541,\n",
       "    'roc_auc': np.float64(0.9466800000000001),\n",
       "    'pr_auc': np.float64(0.8945310014487706),\n",
       "    'thr': 0.36000000000000004}}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.base import clone\n",
    "\n",
    "def resolve_base_key(name: str):\n",
    "    keys = sorted(get_models().keys(), key=len, reverse=True)\n",
    "    for k in keys:\n",
    "        if name.startswith(k):\n",
    "            return k\n",
    "    raise KeyError(f\"No encuentro clave base para '{name}'\")\n",
    "\n",
    "def parse_params_from_name(name: str, base_key: str):\n",
    "    suffix = name[len(base_key):]\n",
    "    if suffix.startswith('_'):\n",
    "        suffix = suffix[1:]\n",
    "    params = {}\n",
    "    if suffix:\n",
    "        for tok in suffix.split('_'):\n",
    "            m = re.match(r'([A-Za-zA-Z]+)([-+]?\\d*\\.?\\d+)$', tok)\n",
    "            if m:\n",
    "                k, v = m.groups()\n",
    "                v = float(v)\n",
    "                if v.is_integer():\n",
    "                    v = int(v)\n",
    "                params[k] = v\n",
    "    return params\n",
    "\n",
    "def set_params(base_key: str, params: dict):\n",
    "    base = get_models()[base_key]\n",
    "    est = clone(base.estimator)\n",
    "    est.set_params(**params)\n",
    "    return base.set_params(estimator=est)\n",
    "\n",
    "best_single = tune_df.iloc[0] if 'tune_df' in globals() and len(tune_df) else rank_df.iloc[0]\n",
    "print(\"Mejor single (tuning o rank):\", best_single[\"name\"])\n",
    "\n",
    "base_key  = resolve_base_key(best_single[\"name\"])\n",
    "params    = {}\n",
    "\n",
    "if 'tune_df' in globals() and 'params' in tune_df.columns:\n",
    "    row = tune_df[tune_df[\"name\"] == best_single[\"name\"]]\n",
    "    if len(row):\n",
    "        params = row.iloc[0].get(\"params\", {}) or {}\n",
    "if not params:\n",
    "    params = parse_params_from_name(best_single[\"name\"], base_key)\n",
    "\n",
    "model_for_full = set_params(base_key, params) if params else get_models()[base_key]\n",
    "\n",
    "res_full, mdl_full, thr_full, test_proba_full, test_m_full = cv_experiment(\n",
    "    best_single[\"name\"], model_for_full, FOLDS_FULL\n",
    ")\n",
    "print(\"FULL — test_f1_micro:\", round(res_full[\"test_f1_micro\"],4),\n",
    "      \" test_f1_macro:\", round(res_full[\"test_f1_macro\"],4))\n",
    "\n",
    "winner_name, winner_obj, winner_thr, winner_metrics = None, None, None, None\n",
    "if 'm_ens' in globals() and m_ens:\n",
    "    if m_ens[\"f1_macro\"] >= res_full[\"test_f1_macro\"]:\n",
    "        winner_name = \"ensemble_blend\"\n",
    "        winner_obj  = None\n",
    "        winner_thr  = THR_ENS\n",
    "        winner_metrics = m_ens\n",
    "    else:\n",
    "        winner_name = best_single[\"name\"] + \"_full5fold\"\n",
    "        winner_obj  = mdl_full\n",
    "        winner_thr  = thr_full\n",
    "        winner_metrics = test_m_full\n",
    "else:\n",
    "    winner_name = best_single[\"name\"] + \"_full5fold\"\n",
    "    winner_obj  = mdl_full\n",
    "    winner_thr  = thr_full\n",
    "    winner_metrics = test_m_full\n",
    "\n",
    "print(\"GANADOR:\", winner_name, \"| macro-F1:\", round(winner_metrics[\"f1_macro\"],4))\n",
    "\n",
    "win_dir = MODELS_OUT / winner_name\n",
    "win_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if winner_obj is not None:\n",
    "    joblib.dump(winner_obj, win_dir/\"model.joblib\")\n",
    "save_json({c: float(t) for c,t in zip(classes, winner_thr)}, win_dir/\"thresholds.json\")\n",
    "save_json(winner_metrics, REPORTS/f\"{winner_name}_test_metrics.json\")\n",
    "\n",
    "print(\"Artefactos en:\", win_dir)\n",
    "winner_name, winner_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_texts(texts, model_dir, is_ensemble=False, members=None):\n",
    "    \"\"\"\n",
    "    is_ensemble=True -> usa average de probas de miembros 'members' (lista de directorios).\n",
    "    \"\"\"\n",
    "    thr_map = json.load(open(Path(model_dir)/\"thresholds.json\"))\n",
    "    thr = np.array([thr_map[c] for c in classes])\n",
    "\n",
    "    X_new = tfidf.transform(pd.Series(texts).astype(str))\n",
    "\n",
    "    if is_ensemble:\n",
    "        Ps = []\n",
    "        for mdir in members:\n",
    "            mdl = joblib.load(Path(mdir)/\"model.joblib\")\n",
    "            Ps.append(to_proba(mdl, X_new))\n",
    "        proba = np.mean(Ps, axis=0)\n",
    "    else:\n",
    "        mdl = joblib.load(Path(model_dir)/\"model.joblib\")\n",
    "        proba = to_proba(mdl, X_new)\n",
    "\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    out = []\n",
    "    for i, t in enumerate(texts):\n",
    "        labs = [c for j,c in enumerate(classes) if y_pred[i,j]==1]\n",
    "        out.append({\n",
    "            \"text\": (t[:120] + \"...\") if len(t)>120 else t,\n",
    "            \"labels\": labs,\n",
    "            \"proba\": {c: float(proba[i,j]) for j,c in enumerate(classes)}\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "if (MODELS_OUT/winner_name/\"model.joblib\").exists():\n",
    "    predict_texts(\n",
    "        [\"randomized trial on cardiac arrhythmia and stroke risk in diabetic patients\"],\n",
    "        MODELS_OUT/winner_name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad28f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top términos + para clase 'cardiovascular':\n",
      "heart                      2.6502\n",
      "cardiac                    2.5186\n",
      "vascular insights          2.4374\n",
      "vascular                   2.4286\n",
      "markers in                 2.1236\n",
      "myocardial                 1.9043\n",
      "cardiovascular             1.8760\n",
      "cardiomyopathy             1.8027\n",
      "markers                    1.6658\n",
      "aortic                     1.5940\n",
      "valve                      1.5383\n",
      "blood pressure             1.5151\n",
      "cardiac connections        1.4645\n",
      "pressure                   1.4511\n",
      "hypotension                1.4165\n",
      "hypotensive                1.3718\n",
      "insights                   1.3532\n",
      "myocardium                 1.3249\n",
      "hypertensive               1.3134\n",
      "hypertension               1.3075\n",
      "\n",
      "Top términos + para clase 'hepatorenal':\n",
      "renal                      4.3138\n",
      "liver                      2.7520\n",
      "hepatic                    2.3070\n",
      "kidney                     2.2637\n",
      "hepatocellular             1.8282\n",
      "creatinine                 1.6470\n",
      "nephritis                  1.5392\n",
      "proteinuria                1.4443\n",
      "cholestasis                1.4263\n",
      "organ                      1.3381\n",
      "polyuria                   1.3088\n",
      "cholecystitis              1.2939\n",
      "rcc                        1.2785\n",
      "transaminase               1.2597\n",
      "glomerulus                 1.2096\n",
      "wilms                      1.1997\n",
      "urea                       1.1543\n",
      "cholelithiasis             1.1324\n",
      "and renal                  1.1296\n",
      "adrenaline                 1.1241\n",
      "\n",
      "Top términos + para clase 'neurological':\n",
      "brain                      3.0957\n",
      "connection in              2.2856\n",
      "brain insights             2.0681\n",
      "symptoms                   2.0605\n",
      "patterns in                1.9366\n",
      "patterns                   1.7665\n",
      "connection                 1.7305\n",
      "secrets                    1.7100\n",
      "neurological               1.4899\n",
      "neuropathy                 1.3870\n",
      "reveals                    1.3440\n",
      "micrograms                 1.3132\n",
      "neurological perspective   1.2800\n",
      "seizures                   1.2565\n",
      "perspective                1.2541\n",
      "neural                     1.2517\n",
      "meets                      1.1926\n",
      "cognitive                  1.1741\n",
      "cerebral                   1.1548\n",
      "exploring neural           1.0208\n",
      "\n",
      "Top términos + para clase 'oncological':\n",
      "cancer                     3.0105\n",
      "tumor                      2.6706\n",
      "chemotherapy               1.8707\n",
      "cancer and                 1.6436\n",
      "leukemia                   1.6109\n",
      "tumors                     1.5810\n",
      "carcinoma                  1.5774\n",
      "neoplasm                   1.4523\n",
      "carcinogen                 1.3929\n",
      "oncogene                   1.3630\n",
      "metastatic                 1.3516\n",
      "brca1                      1.2459\n",
      "malignant                  1.2431\n",
      "sarcoma                    1.2158\n",
      "myeloid                    1.2111\n",
      "myeloid leukemia           1.1849\n",
      "ovarian                    1.1539\n",
      "cancer pathways            1.1276\n",
      "ovarian cancer             1.1167\n",
      "cell                       1.1004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def top_terms_linear(model, k=20):\n",
    "    \"\"\"Extrae top-k términos por clase a partir de coeficientes (LR, Ridge, LinearSVC).\"\"\"\n",
    "    est = model.estimators_[0] if hasattr(model, \"estimators_\") else None\n",
    "    if est is None or not hasattr(est, \"coef_\"):\n",
    "        return None\n",
    "    vocab = np.array([t for t,_ in sorted(tfidf.vocabulary_.items(), key=lambda x: x[1])])\n",
    "    out = {}\n",
    "    for i, c in enumerate(classes):\n",
    "        coef = model.estimators_[i].coef_.ravel()\n",
    "        idx = np.argsort(coef)[-k:][::-1]\n",
    "        out[c] = list(zip(vocab[idx], coef[idx].round(4)))\n",
    "    return out\n",
    "\n",
    "if (MODELS_OUT/winner_name/\"model.joblib\").exists():\n",
    "    mdl = joblib.load(MODELS_OUT/winner_name/\"model.joblib\")\n",
    "    terms = top_terms_linear(mdl, k=20)\n",
    "    if terms is not None:\n",
    "        for c, lst in terms.items():\n",
    "            print(f\"\\nTop términos + para clase '{c}':\")\n",
    "            for w,v in lst: print(f\"{w:25s} {v:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb7d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: ganador: linsvm_C1_C0.5_full5fold | thresholds: [('cardiovascular', 0.35000000000000003), ('hepatorenal', 0.35000000000000003)] ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, os, time, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPORTS = Path(\"../reports\"); REPORTS.mkdir(exist_ok=True)\n",
    "FIGS    = REPORTS/\"figures\"; FIGS.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_EXP = Path(\"../models/experiments\")\n",
    "\n",
    "if 'winner_name' not in globals():\n",
    "    rank_csv = REPORTS/\"experiments_rank.csv\"\n",
    "    assert rank_csv.exists(), \"No encuentro experiments_rank.csv (corre el ranking primero).\"\n",
    "    rank_df = pd.read_csv(rank_csv).sort_values(\"test_f1_macro\", ascending=False)\n",
    "    winner_name = rank_df.iloc[0][\"name\"]\n",
    "    print(\"Inferido winner_name:\", winner_name)\n",
    "\n",
    "\n",
    "if 'winner_metrics' not in globals():\n",
    "    met_json = REPORTS/f\"{winner_name}_test_metrics.json\"\n",
    "    if Path(met_json).exists():\n",
    "        winner_metrics = json.load(open(met_json))\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No encuentro {met_json}. Ejecuta la celda del finalista FULL.\")\n",
    "\n",
    "\n",
    "win_dir = MODELS_EXP/winner_name\n",
    "thr_json = win_dir/\"thresholds.json\"\n",
    "assert thr_json.exists(), f\"No encuentro {thr_json}. Asegúrate de haber exportado los artefactos del ganador.\"\n",
    "thr_map = json.load(open(thr_json))\n",
    "thr_vec = np.array([thr_map[c] for c in classes])\n",
    "\n",
    "\n",
    "y_test = Y[test_idx] if 'Y' in globals() else None\n",
    "print(\"OK: ganador:\", winner_name, \"| thresholds:\", list(thr_map.items())[:2], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardados:\n",
      " - ..\\reports\\best_model_per_class_metrics.csv\n",
      " - ..\\reports\\best_model_thresholds.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>thr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cardiovascular</td>\n",
       "      <td>0.915842</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>0.970623</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hepatorenal</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.952166</td>\n",
       "      <td>0.937276</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neurological</td>\n",
       "      <td>0.904841</td>\n",
       "      <td>0.971664</td>\n",
       "      <td>0.970511</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oncological</td>\n",
       "      <td>0.854054</td>\n",
       "      <td>0.946680</td>\n",
       "      <td>0.894531</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            class        f1   roc_auc    pr_auc   thr\n",
       "0  cardiovascular  0.915842  0.976815  0.970623  0.35\n",
       "1     hepatorenal  0.867257  0.952166  0.937276  0.35\n",
       "2    neurological  0.904841  0.971664  0.970511  0.46\n",
       "3     oncological  0.854054  0.946680  0.894531  0.36"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "per_class = pd.DataFrame.from_dict(winner_metrics[\"per_class\"], orient=\"index\").reset_index()\n",
    "per_class = per_class.rename(columns={\"index\": \"class\"})\n",
    "per_class = per_class[[\"class\",\"f1\",\"roc_auc\",\"pr_auc\",\"thr\"]].sort_values(\"class\")\n",
    "\n",
    "# CSVs\n",
    "per_class_csv = REPORTS/\"best_model_per_class_metrics.csv\"\n",
    "per_class.to_csv(per_class_csv, index=False)\n",
    "\n",
    "thr_csv = REPORTS/\"best_model_thresholds.csv\"\n",
    "pd.DataFrame({\"class\": classes, \"threshold\": thr_vec}).to_csv(thr_csv, index=False)\n",
    "\n",
    "print(\"Guardados:\")\n",
    "print(\" -\", per_class_csv)\n",
    "print(\" -\", thr_csv)\n",
    "\n",
    "per_class.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cf68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardadas curvas PR/ROC por clase en ..\\reports\\figures\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "\n",
    "\n",
    "proba_available = 'test_proba_full' in globals()\n",
    "if not proba_available and (win_dir/\"model.joblib\").exists():\n",
    "    import joblib\n",
    "    mdl = joblib.load(win_dir/\"model.joblib\")\n",
    "    X_test = tfidf.transform(df.loc[test_idx, \"text_norm\"].astype(str))\n",
    "    test_proba_full = to_proba(mdl, X_test)\n",
    "    proba_available = True\n",
    "\n",
    "if proba_available:\n",
    "    for i, c in enumerate(classes):\n",
    "        y_true = y_test[:, i]\n",
    "        y_score = test_proba_full[:, i]\n",
    "\n",
    "        # PR\n",
    "        p, r, _ = precision_recall_curve(y_true, y_score)\n",
    "        ap = average_precision_score(y_true, y_score)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.plot(r, p, lw=2)\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"PR curve — {c} (AP={ap:.3f})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGS/f\"pr_{c}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # ROC\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.plot(fpr, tpr, lw=2)\n",
    "        plt.plot([0,1], [0,1], linestyle=\"--\", lw=1)\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "        plt.title(f\"ROC curve — {c} (AUC={roc_auc:.3f})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGS/f\"roc_{c}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    print(\"Guardadas curvas PR/ROC por clase en\", FIGS)\n",
    "else:\n",
    "    print(\"No hay probabilidades de test en memoria ni modelo single para recomputar; omito curvas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373f26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado análisis de errores: ..\\reports\\best_model_errors_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'test_proba_full' in globals():\n",
    "    y_pred = (test_proba_full >= thr_vec).astype(int)\n",
    "\n",
    "    err_rows = []\n",
    "    for i, c in enumerate(classes):\n",
    "        yt, yp, ys = y_test[:, i], y_pred[:, i], test_proba_full[:, i]\n",
    "        TP = int(((yt==1) & (yp==1)).sum())\n",
    "        FP = int(((yt==0) & (yp==1)).sum())\n",
    "        FN = int(((yt==1) & (yp==0)).sum())\n",
    "        TN = int(((yt==0) & (yp==0)).sum())\n",
    "\n",
    "        \n",
    "        fp_idx = np.where((yt==0) & (yp==1))[0]\n",
    "        fn_idx = np.where((yt==1) & (yp==0))[0]\n",
    "        \n",
    "        top_fp = fp_idx[np.argsort(-ys[fp_idx])][:3]\n",
    "        top_fn = fn_idx[np.argsort(ys[fn_idx])[:3]]\n",
    "\n",
    "        err_rows.append({\n",
    "            \"class\": c, \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "            \"top_fp_examples\": [int(test_idx[j]) for j in top_fp.tolist()],\n",
    "            \"top_fn_examples\": [int(test_idx[j]) for j in top_fn.tolist()],\n",
    "        })\n",
    "\n",
    "    err_df = pd.DataFrame(err_rows)\n",
    "    err_csv = REPORTS/\"best_model_errors_summary.csv\"\n",
    "    err_df.to_csv(err_csv, index=False)\n",
    "    print(\"Guardado análisis de errores:\", err_csv)\n",
    "else:\n",
    "    print(\"Sin probabilidades de test -> omito análisis de errores (Celda C puede recomputarlas si el modelo es single).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: ..\\reports\\modeling_report.md\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "def df_to_md_safe(df):\n",
    "    try:\n",
    "        return df.to_markdown(index=False) \n",
    "    except Exception:\n",
    "        \n",
    "        header = \"| \" + \" | \".join(df.columns.astype(str)) + \" |\"\n",
    "        sep    = \"| \" + \" | \".join([\"---\"]*len(df.columns)) + \" |\"\n",
    "        rows   = [\"| \" + \" | \".join(map(str, r)) + \" |\" for r in df.astype(str).values]\n",
    "        return \"\\n\".join([header, sep] + rows)\n",
    "\n",
    "\n",
    "n_rows = len(df)\n",
    "label_counts = {c: int(df[c].sum()) for c in classes}\n",
    "\n",
    "\n",
    "rank_path = REPORTS/\"experiments_rank.csv\"\n",
    "rank_md = \"\"\n",
    "if rank_path.exists():\n",
    "    rank_df = pd.read_csv(rank_path).sort_values(\"test_f1_macro\", ascending=False)\n",
    "    rank_md = df_to_md_safe(rank_df)\n",
    "\n",
    "\n",
    "wm = winner_metrics\n",
    "per_class = pd.DataFrame.from_dict(wm[\"per_class\"], orient=\"index\").reset_index().rename(columns={\"index\":\"class\"})\n",
    "per_class = per_class[[\"class\",\"f1\",\"roc_auc\",\"pr_auc\",\"thr\"]].sort_values(\"class\")\n",
    "per_class_md = df_to_md_safe(per_class)\n",
    "\n",
    "md = f\"\"\"# Modeling Report — {winner_name}\n",
    "**Fecha:** {datetime.now():%Y-%m-%d %H:%M}\n",
    "\n",
    "## 1) Resumen ejecutivo\n",
    "- **Ganador:** `{winner_name}`\n",
    "- **F1 micro (test):** {wm[\"f1_micro\"]:.4f}\n",
    "- **F1 macro (test):** {wm[\"f1_macro\"]:.4f}\n",
    "- **ROC-AUC macro (test):** {wm[\"roc_auc_macro\"]:.4f}\n",
    "- **PR-AUC macro (test):** {wm[\"pr_auc_macro\"]:.4f}\n",
    "\n",
    "## 2) Datos\n",
    "- Filas: **{n_rows}**\n",
    "- Clases (**{len(classes)}**): {\", \".join(classes)}\n",
    "- Positivos por clase: {label_counts}\n",
    "\n",
    "## 5) Ranking de modelos (top)\n",
    "{rank_md if rank_md else \"_No se encontró experiments_rank.csv_\"}\n",
    "\n",
    "## 6) Métricas por clase (ganador)\n",
    "{per_class_md}\n",
    "\n",
    "## 7) Umbrales por clase\n",
    "Archivo: `reports/best_model_thresholds.csv`.\n",
    "\"\"\"\n",
    "out_md = REPORTS/\"modeling_report.md\"\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md)\n",
    "print(\"Guardado:\", out_md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
